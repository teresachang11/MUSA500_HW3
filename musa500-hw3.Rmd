---
title: "MUSA500 - HW3"
author: "Chenxi Zhu, Teresa Chang, and Tiffany Luo "
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
editor_options: 
  markdown: 
    wrap: 80
  chunk_output_type: inline
---

# Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol

## 1 Introduction

## 2 Methodology

### 2.3 Hypothesis

Here, we are doing the following hypothesis test for each predictor $X_ð‘–$:

-   $ð»_0: ð›½_ð‘–=0$ The null hypothesis ($ð»_0$â€‹) is that the predictor $X_i$â€‹ has no
    effect on the probability of the dependent event occurring.

-   $ð»_ð‘Ž: ð›½_ð‘–â‰ 0$ The alternative hypothesis ($ð»_a$â€‹) posits that the predictor
    $X_i$â€‹â€‹ does have an effect on the probability of the dependent event
    occurring.

The test statistic used to evaluate these hypotheses is called Wald statistic
within the context of logistic regression. It is given by the formula
$W = \hatÎ²Ì‚_i / \sigma_{\hatÎ²Ì‚_i}$ and has a normal distribution, where $\hatÎ²Ì‚_i$
is the estimated coefficient for the ith predictor and $\sigma_{\hatÎ²Ì‚_i}$ is the
variance of the estimated coefficient.

Rather than looking at the estimated ð›½ coefficients, most statisticians prefer
to look at odds ratios, which are calculated by exponentiating the coefficients
$OR_i=e^{\hatÎ²_â€‹i}$. Therefore, the hypothesis can also be written as
$ð»_0: ð‘‚ð‘…_ð‘–=1$ and $ð»_ð‘Ž:ð‘‚ð‘…_ð‘–â‰ 1$.

### 2.4 Assessing the quality of model fit

The quality of the logistic regression model fit is not well described by $R^2$
as in OLS. $R^2$ in OLS regression is a direct measure of variance explained, in
logistic regression it is a more abstract concept that requires careful
interpretation and is often replaced by other measures of model fit.

**Akaike Information Criterion** (AIC) is a measure for comparing the relative
quality of statistical models for a given dataset, penalizing complexity to
favor models that achieve a good fit with fewer parameters. A lower AIC
indicates a model with a better balance between simplicity and fit.

-   **Sensitivity**: The proportion of true positives correctly identified, with
    higher values indicating better model performance.

-   **Specificity**: The proportion of true negatives correctly identified, with
    higher values also being preferable.

-   **Misclassification Rate**: The proportion of incorrect predictions, where a
    lower rate signifies a more accurate model.

The fitted values in logistic regression, $\hat y$ are the predicted
probabilities that the outcome variable equals one. These probabilities are
calculated using the logistic function, and a cut-off value is chosen to
classify these probabilities into binary outcomes. Different cut-offs for the
probability are considered to balance sensitivity and specificity, which can
vary depending on the context and the costs associated with false positives and
false negatives. Adjusting the cut-off can optimize the model for either recall
or precision, depending on which is more critical for the given application.

The **Receiver Operating Characteristic** (ROC) curve plots the true positive
rate (sensitivity) against the false positive rate (1-specificity) at various
threshold settings, providing a visual representation of a classifier's
performance across all possible cut-offs. A couple different ways for
identifying the probability cut-offs based on ROC Curves:

-   Youden Index: A cut-off for which (Sensitivity + Specificity) is maximized

-   A cut-off for which the ROC curve has the minimum distance from the upper
    left corner of the graph -- i.e., the point at which specificity = 1 and
    sensitivity = 1.

The **Area Under the ROC Curve** (AUC) is a summary measure that represents a
model's ability to discriminate between positive and negative classes. The AUC
ranges from 0 to 1, with higher values indicating better model performance. A
typical classification scheme is:

-   Excellent: AUC \> 0.9

-   Good: AUC between 0.8 and 0.9

-   Fair: AUC between 0.7 and 0.8

-   Poor: AUC between 0.6 and 0.7

-   Fail: AUC \< 0.6

In essence, these metrics provide a comprehensive view of the model's predictive
accuracy and are essential for both model evaluation and decision-making
processes.

### 2.5 Assumptions of Logistic Regression

Comparing assumptions with OLS:

**Shared Assumptions:**

-   **Independence of observations:** Both OLS and logistic regression require
    that observations be independent of one another.

-   **No severe multicollinearity:** Both types of regression assume that
    predictors are not perfectly collinear; however, logistic regression is
    generally more tolerant of multicollinearity.

**Distinct Assumptions of Logistic Regression:**

-   **Binary Dependent Variable (DV):** The dependent variable in logistic
    regression must be binary.

-   **Larger sample Size:** Logistic regression typically requires a larger
    sample size due to the use of Maximum Likelihood Estimation (MLE) rather
    than least squares for estimating regression coefficients. A rule of thumb
    is at least 50 observations per predictor, compared to about 10 per
    predictor in OLS regression.

**Assumptions Not Required for Logistic Regression:**

-   **Linearity:** There is no requirement for a linear relationship between the
    DV and the independent variables (IVs). Instead, logistic regression assumes
    linearity between the log odds of the DV and each IV.

-   **Homoscedasticity and Normality of Residuals:** Unlike OLS, logistic
    regression does not assume that residuals are normally distributed or that
    the variance of error terms is constant across all levels of the IVs.

### 2.6 Exploratory Analyses

Before performing logistic regression, statisticians often engage in exploratory
analyses to investigate the structure and relationships within the data.

#### 2.6.1 Cross-Tabulations & Chi-Square Test

Cross-tabulations are utilized to examine the relationship between the binary
dependent variable `DRIVING_D` and each binary predictor (`FATAL_OR_M`,
`OVERTURNED`, `CELL_PHONE`, `SPEEDING`, `AGGRESSIVE`, `DRIVER1617`,
`DRIVER65PLUS`). The Chi-Square test is the appropriate statistical test for
assessing the association between two categorical variables. The Chi-Square
distribution is the sum of squares of $k$ independent standard normal random
variables, where $k$ is the degrees of freedom. Degrees of freedom can be
calculated as $(R-1)(C-1)$ where $C$ is the number of categories in one of the
variables and $R$ is the number of categories in the other variable. The
hypotheses are:

-   $H_0$ : the proportion of drinking driver across the variable categories are
    the same.

-   $H_a$ : the proportion of drinking driver across the variable categories are
    not the same.

#### 2.6.2 Comparing Means of Continuous Predictors

For continuous predictors (`PCTBACHMR`, `MEDHHINC`), we can compare the means
across the different values of the dependent variable. This is typically done
using independent samples t-tests. For example, we can see whether the average
`PCTBACHMOR` values are statistically significantly different for crashes that
involve drunk drivers and crashes that don't. The null and alternative
hypotheses for the independent samples t-test would be as follows:

-   $H_0$ : average values of the variable `PCTBACHMOR` are the same for crashes
    that involve drunk drivers and crashes that don't.

-   $H_a$ : average values of the variable `PCTBACHMOR` are different for
    crashes that involve drunk drivers and crashes that don't.

## 3 Results

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(janitor)
#library(ggthemr)
library(ggpubr)
library(ggrepel)
library(purrr)
library(kableExtra)
library(caret)
library(corrplot)
library(prediction)
options(scipen = 999) # turn off scientific notation
library(aod)
#library(rms)
library(gmodels)
library(ROCR)
library(crosstable)
library(flextable)
library(ggplot2)

#import data
library(readr)
crash_data <- read_csv("Logistic Regression Data.csv")
```

### 3.1 Exploratory Analyses

```{r}
table_result <- table(factor(crash_data$DRINKING_D, levels = c(0, 1), labels = c("No Alcohol Involved", "Alcohol Involved")))


result_df <- as.data.frame(table_result)
result_df$Proportion <- prop.table(table_result)
colnames(result_df) <- c("Alcoho-related", "Count", "Proportion")

#print(result_df)
kable(result_df)

```

```{r}
variables <- c("FATAL_OR_M", "OVERTURNED", "CELL_PHONE", "SPEEDING", "AGGRESSIVE", "DRIVER1617", "DRIVER65PLUS")

create_and_print_table <- function(variable) {
  crosstable(crash_data, !!as.name(variable), by = "DRINKING_D", funs = "mean", test = TRUE) %>% 
    as_flextable() 
}

tables <- map(variables, create_and_print_table)
tables
```

```{r}
# Assuming your data frame is named 'crash_data' and the dependent variable is named 'DRINKING_D'
# Replace these variable names with the actual names in your dataset

# Subset the data into two groups
group1 <- crash_data[crash_data$DRINKING_D == 0, ]
group2 <- crash_data[crash_data$DRINKING_D == 1, ]

# Calculate means and standard deviations for both groups and both predictors
mean_group1_var1 <- mean(group1[['PCTBACHMOR']])
mean_group1_var2 <- mean(group1[['MEDHHINC']])
mean_group2_var1 <- mean(group2[['PCTBACHMOR']])
mean_group2_var2 <- mean(group2[['MEDHHINC']])
sd_group1_var1 <- sd(group1[['PCTBACHMOR']])
sd_group1_var2 <- sd(group1[['MEDHHINC']])
sd_group2_var1 <- sd(group2[['PCTBACHMOR']])
sd_group2_var2 <- sd(group2[['MEDHHINC']])

# Perform t-tests for both predictors
t_test_result_var1 <- t.test(group1[['PCTBACHMOR']], group2[['PCTBACHMOR']])
t_test_result_var2 <- t.test(group1[['MEDHHINC']], group2[['MEDHHINC']])

# Create a summary data frame
summary_df <- data.frame(
  Variable = c("PCTBACHMOR", "MEDHHINC"),
  Non_drunk_Mean = c(mean_group1_var1, mean_group1_var2),
  Non_drunk_SD = c(sd_group1_var1, sd_group1_var2),
  Drunk_Mean = c(mean_group2_var1, mean_group2_var2),
  Drunk_SD = c(sd_group2_var1, sd_group2_var2),
  P_Value_PCTBACHMOR = t_test_result_var1$p.value,
  P_Value_MEDHHINC = t_test_result_var2$p.value
)

# Print the summary table
#print(summary_df)

kable(summary_df, format = "html", digits = c(2, 2, 2, 2, 4, 4), 
      caption = "Summary Table of Means, Standard Deviations, and T-Test Results")

```

### 3.2 Logistic Regression Assumptions

```{r}
correlation <- crash_data[c(4:10, 12:13)]
cor(correlation, method = "pearson")
```

```{r, warning=FALSE}
library(corrr)

correlation %>% 
  correlate() %>% 
  autoplot() +
  geom_text(aes(label = round(r, digits=2)), size =3) +
  labs(title = "COrrelation Matrix")
```

### 3.3 Logistic Regression Results

```{r}
logistic <- glm(DRINKING_D ~ FATAL_OR_M + OVERTURNED + CELL_PHONE +
                        SPEEDING + AGGRESSIVE + DRIVER1617 + DRIVER65PLUS +
                        PCTBACHMOR + MEDHHINC, family = "binomial", data = crash_data)

# Display summary
summary(logistic)

```

```{r}
logitoutput <- summary(logistic)
logitcoeffs <- logitoutput$coefficients
logitcoeffs
```

```{r}
or_ci <- exp(cbind(OR = coef(logistic), confint(logistic)))

finallogitoutput <- cbind(logitcoeffs, or_ci)
finallogitoutput
```

```{r}
# Assuming your logistic regression model is named 'logistic' and 'cut_off_values' is a vector of cut-off values
# Replace these variable names with the actual names in your dataset

# Specify cut-off values
cut_off_values <- c(0.02, 0.03, 0.05, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.5)

# Initialize a data frame to store results
result_df <- data.frame(
  Cut_off_Value = cut_off_values,
  Sensitivity = numeric(length(cut_off_values)),
  Specificity = numeric(length(cut_off_values)),
  Misclassification_Rate = numeric(length(cut_off_values))
)

# Extract predicted probabilities from the logistic model
fit <- predict(logistic, type = "response")

# Loop through each cut-off value and calculate metrics
for (i in seq_along(cut_off_values)) {
  fit.binary <- as.integer(fit >= cut_off_values[i])
  table_result <- table(fit.binary, crash_data$DRINKING_D)
  
  # Check if both classes are present in the data
  if (sum(table_result[1, ]) > 0 && sum(table_result[2, ]) > 0) {
    # Extract metrics from the table_result object
    sensitivity <- table_result[2, 2] / sum(table_result[2, ])
    specificity <- table_result[1, 1] / sum(table_result[1, ])
    misclassification_rate <- sum(table_result[1, 2], table_result[2, 1]) / sum(table_result)
    
    # Store results in the data frame
    result_df[i, c("Sensitivity", "Specificity", "Misclassification_Rate")] <- c(sensitivity, specificity, misclassification_rate)
  } else {
    warning(paste("One or both classes missing for cut-off value:", cut_off_values[i]))
  }
}

# Print the resulting data frame
kable(result_df)


```

```{r}
a <- cbind(crash_data$DRINKING_D, fit)
colnames(a) <- c("labels", "predictions")
head(a)
```

```{r}
roc <- as.data.frame(a)
pred <- ROCR::prediction(roc$predictions, roc$labels)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf)
abline(a = 0, b = 1)
```

```{r}
opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

print(opt.cut(roc.perf, pred))
```

```{r}
auc.perf = performance(pred, measure ="auc")
auc.perf@y.values

```

**2**

```{r}
logistic2 <- glm(DRINKING_D ~ FATAL_OR_M + OVERTURNED + CELL_PHONE +
                        SPEEDING + AGGRESSIVE + DRIVER1617 + DRIVER65PLUS, family = "binomial", data = crash_data)

# Display summary
summary(logistic2)

```

```{r}
logitoutput <- summary(logistic2)
logitcoeffs <- logitoutput$coefficients
logitcoeffs
```

```{r}
or_ci <- exp(cbind(OR = coef(logistic2), confint(logistic2)))

finallogitoutput <- cbind(logitcoeffs, or_ci)
finallogitoutput
```

```{r}
AIC(logistic, logistic2) %>%
  kbl(caption = "AIC") %>%
  kable_styling() %>%
  kable_classic(html_font = "Cambria", position = "left", full_width = F)
```

## 4 Discussion

## 5 Citation
